apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: sparkapp-start-sensor
  namespace: argo
spec:
  template:
    serviceAccountName: operate-workflow-sa
  dependencies:
    - name: sparkapp-start
      eventSourceName: spark-webhook
      eventName: sparkapp-start
  triggers:
    - template:
        name: sparkapp-workflow-trigger
        serviceAccountName: operate-workflow-sa
        argoWorkflow:
          group: argoproj.io
          version: v1alpha1
          resource: workflows
          operation: submit
          parameters:
            - src:
                dependencyName: sparkapp-start
                dataTemplate: "{{ .Input.body.script }}"
              dest: spec.arguments.parameters.0.value
            - src:
                dependencyName: sparkapp-start
                dataTemplate: "{{ .Input.body.pvc }}"
              dest: spec.arguments.parameters.1.value
            - src:
                dependencyName: sparkapp-start
                dataTemplate: "{{ .Input.body.path }}"
              dest: spec.arguments.parameters.2.value
            - src:
                dependencyName: sparkapp-start
                dataTemplate: "{{ .Input.body.executor }}"
              dest: spec.arguments.parameters.3.value
            - src:
                dependencyName: sparkapp-start
                dataTemplate: "{{ .Input.body.ak }}"
              dest: spec.arguments.parameters.4.value
            - src:
                dependencyName: sparkapp-start
                dataTemplate: "{{ .Input.body.sk }}"
              dest: spec.arguments.parameters.5.value
            - src:
                dependencyName: sparkapp-start
                dataTemplate: "{{ .Input.body.endpoint }}"
              dest: spec.arguments.parameters.6.value
            - src:
                dependencyName: sparkapp-start
                dataTemplate: "{{ .Input.body.bucket }}"
              dest: spec.arguments.parameters.7.value
          source:
            resource:
              apiVersion: argoproj.io/v1alpha1
              kind: Workflow
              metadata:
                generateName: sparkapp-workflow-
                namespace: spark
              spec:
                entrypoint: sparkapp-operator
                ttlStrategy:
                  secondsAfterCompletion: 3600
                arguments:
                  parameters:
                    - name: script
                    - name: pvc
                    - name: path
                    - name: executor
                    - name: ak
                    - name: sk
                    - name: endpoint
                    - name: bucket
                templates:
                  - name: sparkapp-operator
                    resource:
                      action: create
                      manifest: |
                        apiVersion: "sparkoperator.k8s.io/v1beta2"
                        kind: SparkApplication
                        metadata:
                          generateName: sparkapp-
                          namespace: spark
                        spec:
                          type: Python
                          pythonVersion: "3"
                          mode: cluster
                          image: shaowenchen/spark-k8s:py-s3
                          imagePullPolicy: Always
                          mainApplicationFile: local:///data-spark/{{workflow.parameters.script}}
                          volumes:
                          - name: data
                            persistentVolumeClaim:
                              claimName: '{{workflow.parameters.pvc}}'
                          sparkConf:
                            spark.sql.extensions: org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
                            spark.sql.catalog.demo: org.apache.iceberg.spark.SparkCatalog
                            spark.sql.catalog.demo.type: hive
                            spark.sql.catalog.demo.warehouse: s3a://{{workflow.parameters.bucket}}/datalake/spark-warehouse
                            spark.sql.defaultCatalog: demo
                            spark.sql.hive.thriftServer.singleSession: "false"
                            spark.hive.metastore.schema.verification: "false"
                            spark.hive.metastore.uris: thrift://hive-metastore.hive.svc:9083
                            spark.hadoop.fs.s3.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
                            spark.hadoop.fs.s3n.impl: org.apache.hadoop.fs.s3a.S3AFileSystem
                            spark.hadoop.fs.s3a.path.style.access: "false"
                            spark.hadoop.fs.s3a.endpoint: {{workflow.parameters.endpoint}}
                            spark.hadoop.fs.s3a.bucket.name: {{workflow.parameters.bucket}}
                            spark.hadoop.fs.s3a.access.key: {{workflow.parameters.ak}}
                            spark.hadoop.fs.s3a.secret.key: {{workflow.parameters.sk}}
                            spark.eventLog.enabled: "false"
                          sparkVersion: "3.5.1"
                          restartPolicy:
                            type: OnFailure
                            onFailureRetries: 3
                            onFailureRetryInterval: 10
                            onSubmissionFailureRetries: 5
                            onSubmissionFailureRetryInterval: 20
                          driver:
                            cores: 4
                            memory: 8G
                            labels:
                              version: 3.5.1
                            serviceAccount: default
                            volumeMounts:
                            - name: data
                              mountPath: /data-spark
                              subPath: '{{workflow.parameters.path}}'
                          executor:
                            cores: 32
                            memory: 64G
                            instances: {{workflow.parameters.executor}}
                            labels:
                              version: 3.5.1
                            volumeMounts:
                            - name: data
                              mountPath: /data-spark
                              subPath: '{{workflow.parameters.path}}'
